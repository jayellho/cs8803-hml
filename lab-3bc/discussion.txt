Answer the following questions for part B-2, based on the results from B-1:
(a) Which parallelization strategies achieve the highest and lowest speedup compared to a single GPU? Explain why these strategies result in the highest or lowest speedup.
Ans: Data parallelism with 2 GPUs achieves the highest speedup, while pipeline parallelism with 2 GPUs and activation recomputation had the lowest speedup. Data parallelism provides the most speedup because the separate GPUs perform computation with minimal synchronization and communication and hence produces the best speedup. Pipeline parallelism provides the least speedup because there is additional computational cost incurred in recomputing the activations (used to save memory). In addition, there may have been pipeline bubbles which increase wait times and hence overall runtimes.

(b) Some parallelization strategies provide minimal speedup over a single GPU case. Is there any reason to use such a strategy instead of a single GPU?
Ans: Yes, in cases when the model or data is too big to fit on a single GPU, it would make sense to use in place of a single GPU - this means that the size of the model or data is also not limited to the size of a single GPU, thus helping with scalability. With methods like activation recomputation, it may help to stabilize training as it helps to reduce memory usage (and hence prevent out-of-memory errors), even when speedup is not a lot.

(c) In an ideal system where communication latency is zero, what would be the speedup of data parallelism and pipeline parallelism compared to a single GPU?
Ans: For data parallelism, the speedup would be the number of GPUs used since there is no overhead for synchronizing gradients and each GPU processes the workload concurrently. For pipeline parallelism, assuming zero communication latency and perfect scheduling with no pipeline bubbles, the speedup would be also the number of GPUs used, since the GPUs are fully utilized concurrently. However, if the perfect scheduling assumption does not hold, the speedup would be less than the number of GPUs used, depending on how many and how big pipeline bubbles there are.

(d) Analyze the traces of two GPU workers using the tensor-parallel strategy. Did they execute the same sequence of operators? If not, what differences exist between the two workers, and what causes them?
Ans: Looking at the traces, the sequence of operators seems to be mostly the same, and this is consistent with understanding of tensor parallelism since each GPU processes the same portion of the model in parallel so the kernels should be mostly identical.

(e) Analyze the traces of two GPU workers using the pipeline-parallel strategy. Did they execute the same sequence of operators? If not, what differences exist between the two workers, and what causes them?
Ans: No. For pipeline parallelism, workers execute different stages of the pipeline and hence there are differences in the sequences of operators. Looking at the traces, GPU worker 0 runs the forward pass, then the nccl:coalesced and ncclDevKernel_SendRecv, whereas for GPU worker 1, ncclDevKernel_SendRecv and nccl:coalesced are run first, followed by the Forward Pass and other operators before ending with nccl:all_reduce and ncclDevKernel_AllReduce. This is consistent with understanding of how pipeline parallelism works.
