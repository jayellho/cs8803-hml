{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5282ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 28 22:57:13 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          On  |   00000000:BE:00.0 Off |                    0 |\n",
      "| N/A   33C    P0            104W /  700W |       1MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA H100 80GB HBM3          On  |   00000000:C7:00.0 Off |                    0 |\n",
      "| N/A   40C    P0             86W /  700W |       1MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check if you can see 2 H100 GPUs here\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a1c0b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find free port starting from init\n",
    "def find_freeport(init):\n",
    "    import socket\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    port = init\n",
    "    while True:\n",
    "        try:\n",
    "            s.bind((\"localhost\", port))\n",
    "            break\n",
    "        except:\n",
    "            port += 1\n",
    "    s.close()\n",
    "    return port"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5491b4",
   "metadata": {},
   "source": [
    "## Part B-0: Single-GPU training\n",
    "\n",
    "The following cell executes a GPT3-like model with 2 layers and a batch size of 4 on a single GPU. You have a starting reference config file for a single GPU (megatron_configs/single_gpu.yaml). Executing the cell in juypter notebook will train the model for 1 epoch and generate the profiling logs, the trace will be generated on prof_log directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2056f0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Megatron-LM\n",
      "using world size: 1, data-parallel size: 1, context-parallel size: 1 tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  add_bias_linear ................................. True\n",
      "  add_position_embedding .......................... True\n",
      "  add_qkv_bias .................................... False\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_layernorm_1p .............................. False\n",
      "  apply_query_key_layer_scaling ................... False\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  apply_rope_fusion ............................... True\n",
      "  async_tensor_model_parallel_allreduce ........... True\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  barrier_with_L1_time ............................ True\n",
      "  bert_binary_head ................................ True\n",
      "  bert_embedder_type .............................. megatron\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  bias_swiglu_fusion .............................. True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  check_for_nan_in_loss_and_grad .................. True\n",
      "  classes_fraction ................................ 1.0\n",
      "  clip_grad ....................................... 1.0\n",
      "  clone_scatter_output_in_embedding ............... True\n",
      "  consumed_samples ................................ None\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  context_parallel_size ........................... 1\n",
      "  data_cache_path ................................. None\n",
      "  data_parallel_random_init ....................... False\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... None\n",
      "  data_per_class_fraction ......................... 1.0\n",
      "  data_sharding ................................... True\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ torch\n",
      "  decoder_num_layers .............................. None\n",
      "  decoder_seq_length .............................. None\n",
      "  delay_grad_reduce ............................... True\n",
      "  delay_param_gather .............................. False\n",
      "  dino_bottleneck_size ............................ 256\n",
      "  dino_freeze_last_layer .......................... 1\n",
      "  dino_head_hidden_size ........................... 2048\n",
      "  dino_local_crops_number ......................... 10\n",
      "  dino_local_img_size ............................. 96\n",
      "  dino_norm_last_layer ............................ False\n",
      "  dino_teacher_temp ............................... 0.07\n",
      "  dino_warmup_teacher_temp ........................ 0.04\n",
      "  dino_warmup_teacher_temp_epochs ................. 30\n",
      "  distribute_saved_activations .................... False\n",
      "  distributed_backend ............................. nccl\n",
      "  distributed_timeout_minutes ..................... 10\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_one_logger ............................... False\n",
      "  encoder_num_layers .............................. 2\n",
      "  encoder_seq_length .............................. 2048\n",
      "  end_weight_decay ................................ 0.01\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 100\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... None\n",
      "  exit_on_missing_checkpoint ...................... False\n",
      "  exit_signal_handler ............................. False\n",
      "  expert_model_parallel_size ...................... 1\n",
      "  ffn_hidden_size ................................. 10240\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  fp8 ............................................. None\n",
      "  fp8_amax_compute_algo ........................... most_recent\n",
      "  fp8_amax_history_len ............................ 1\n",
      "  fp8_interval .................................... 1\n",
      "  fp8_margin ...................................... 0\n",
      "  fp8_wgrad ....................................... True\n",
      "  global_batch_size ............................... 4\n",
      "  gradient_accumulation_fusion .................... True\n",
      "  group_query_attention ........................... False\n",
      "  head_lr_mult .................................... 1.0\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2560\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_h ........................................... 224\n",
      "  img_w ........................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  inference_batch_times_seqlen_threshold .......... 512\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  iter_per_epoch .................................. 1250\n",
      "  kv_channels ..................................... 80\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ None\n",
      "  local_rank ...................................... None\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 100\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_progress .................................... False\n",
      "  log_throughput .................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  log_world_size_to_tensorboard ................... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 5e-05\n",
      "  lr_decay_iters .................................. None\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. linear\n",
      "  lr_mult ......................................... 1.0\n",
      "  lr_warmup_fraction .............................. None\n",
      "  lr_warmup_init .................................. 0.0\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  manual_gc ....................................... False\n",
      "  manual_gc_eval .................................. True\n",
      "  manual_gc_interval .............................. 0\n",
      "  mask_factor ..................................... 1.0\n",
      "  mask_prob ....................................... 0.15\n",
      "  mask_type ....................................... random\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 2048\n",
      "  max_tokens_to_oom ............................... 12000\n",
      "  megatron_dataset_flag ........................... False\n",
      "  merge_file ...................................... /home/hice1/jho88/lab3B/src/merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 0.0\n",
      "  mock_data ....................................... False\n",
      "  model_return_dict ............................... True\n",
      "  model_type_name ................................. gpt\n",
      "  moe_aux_loss_coeff .............................. 0.0\n",
      "  moe_grouped_gemm ................................ False\n",
      "  moe_input_jitter_eps ............................ None\n",
      "  moe_router_load_balancing_type .................. aux_loss\n",
      "  moe_router_topk ................................. 2\n",
      "  moe_token_dropping .............................. False\n",
      "  moe_z_loss_coeff ................................ None\n",
      "  nccl_communicator_config_path ................... None\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_persist_layer_norm ........................... False\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  no_wd_decay_cond ................................ None\n",
      "  norm_epsilon .................................... 1e-05\n",
      "  normalization ................................... LayerNorm\n",
      "  num_attention_heads ............................. 32\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_experts ..................................... None\n",
      "  num_layers ...................................... 2\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_micro_batches ............................... 1\n",
      "  num_query_groups ................................ 1\n",
      "  num_workers ..................................... 2\n",
      "  one_logger_entity ............................... hwinf_dcm\n",
      "  one_logger_project .............................. e2e-tracking\n",
      "  one_logger_run_name ............................. None\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  orig_vocab_size ................................. 50257\n",
      "  output_bert_embeddings .......................... False\n",
      "  overlap_grad_reduce ............................. False\n",
      "  overlap_p2p_comm ................................ False\n",
      "  overlap_param_gather ............................ False\n",
      "  override_opt_param_scheduler .................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  perform_initialization .......................... True\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  position_embedding_type ......................... learned_absolute\n",
      "  pretraining_flag ................................ True\n",
      "  profile ......................................... False\n",
      "  profile_ranks ................................... [0]\n",
      "  profile_step_end ................................ 12\n",
      "  profile_step_start .............................. 10\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  recompute_granularity ........................... None\n",
      "  recompute_method ................................ None\n",
      "  recompute_num_layers ............................ None\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  retro_add_retriever ............................. False\n",
      "  retro_attention_gate ............................ 1\n",
      "  retro_cyclic_train_iters ........................ None\n",
      "  retro_encoder_attention_dropout ................. 0.1\n",
      "  retro_encoder_hidden_dropout .................... 0.1\n",
      "  retro_encoder_layers ............................ 2\n",
      "  retro_num_neighbors ............................. 2\n",
      "  retro_num_retrieved_chunks ...................... 2\n",
      "  retro_return_doc_ids ............................ False\n",
      "  retro_verify_neighbor_count ..................... True\n",
      "  retro_workdir ................................... None\n",
      "  return_logits ................................... False\n",
      "  rotary_interleaved .............................. False\n",
      "  rotary_percent .................................. 1.0\n",
      "  rotary_seq_len_interpolation_factor ............. None\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ None\n",
      "  save_interval ................................... None\n",
      "  scale_lr_cond ................................... None\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 2048\n",
      "  sequence_parallel ............................... False\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  skip_train ...................................... False\n",
      "  spec ............................................ None\n",
      "  split ........................................... 969, 30, 1\n",
      "  squared_relu .................................... False\n",
      "  standalone_embedding_stage ...................... False\n",
      "  start_weight_decay .............................. 0.01\n",
      "  swiglu .......................................... False\n",
      "  swin_backbone_type .............................. tiny\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  test_data_path .................................. None\n",
      "  timing_log_level ................................ 0\n",
      "  timing_log_option ............................... minmax\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_model ................................. None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  tp_comm_bulk_dgrad .............................. True\n",
      "  tp_comm_bulk_wgrad .............................. True\n",
      "  tp_comm_overlap ................................. False\n",
      "  tp_comm_overlap_cfg ............................. None\n",
      "  tp_comm_split_ag ................................ True\n",
      "  tp_comm_split_rs ................................ True\n",
      "  train_data_path ................................. None\n",
      "  train_iters ..................................... 1\n",
      "  train_samples ................................... None\n",
      "  transformer_impl ................................ local\n",
      "  transformer_pipeline_model_parallel_size ........ 1\n",
      "  untie_embeddings_and_output_weights ............. False\n",
      "  use_checkpoint_args ............................. False\n",
      "  use_checkpoint_opt_param_scheduler .............. False\n",
      "  use_cpu_initialization .......................... None\n",
      "  use_distributed_optimizer ....................... False\n",
      "  use_flash_attn .................................. False\n",
      "  use_mcore_models ................................ False\n",
      "  use_one_sent_docs ............................... False\n",
      "  use_ring_exchange_p2p ........................... False\n",
      "  use_rotary_position_embeddings .................. False\n",
      "  valid_data_path ................................. None\n",
      "  variable_seq_lengths ............................ False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vision_backbone_type ............................ vit\n",
      "  vision_pretraining .............................. False\n",
      "  vision_pretraining_type ......................... classify\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/hice1/jho88/lab3B/src/vocab.json\n",
      "  vocab_size ...................................... None\n",
      "  wandb_exp_name .................................. \n",
      "  wandb_project ................................... \n",
      "  wandb_save_dir .................................. \n",
      "  weight_decay .................................... 0.01\n",
      "  weight_decay_incr_style ......................... constant\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "torch distributed is already initialized, skipping initialization ...\n",
      "> initialized tensor model parallel with size 1\n",
      "> initialized pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> compiling dataset index builder ...\n",
      "make: Entering directory '/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/datasets'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/datasets'\n",
      ">>> done with dataset index builder. Compilation time: 0.041 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 0.309 seconds\n",
      "Preparing model optimizer scheduler\n",
      "Building gpt model in the pre-training mode.\n",
      "The Megatron LM model weights are initialized at random in `accelerator.prepare`. Please use `accelerator.load_checkpoint` to load a pre-trained checkpoint matching the distributed setup.\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 291379200\n",
      "> learning rate decay style: linear\n",
      "Preparing dataloader\n",
      "---------------------------------------\n",
      "----- Profiling done successfully -----\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch --main_process_port {find_freeport(25900)} --config_file \"megatron_configs/single_gpu.yaml\" src/prof.py --model_name \"model_configs/gpt3_27_2_layer.json\" --total_batch_size \"4\" --logdir \"prof_log/single_gpu\" 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc74f1be",
   "metadata": {},
   "source": [
    "## Part B-1: Analyzing Multi-GPU training\n",
    "\n",
    "Next, we will try to train the model using multiple GPUs. We use [Megatron-LM](https://huggingface.co/docs/accelerate/usage_guides/megatron_lm) for generating distributed training runs.\n",
    "\n",
    "Write new Configuration files and store them in the megatron_config directory (megatron_config/*.yaml). <br>\n",
    "**For each run, change the argument --config_file and --log_dir appropriately**\n",
    "\n",
    "<ins> You need to change the following parameters. Refer to megatron-LM link for understanding of how the three parameters are used.<ins> <br>\n",
    "\n",
    "1.\tmegatron_lm_pp_degree (PP) <br>\n",
    "2.\tmegatron_lm_tp_degree (TP) <br>\n",
    "3.\tmegatron_lm_recompute_activations (AR) <br>\n",
    "\n",
    "<ins>Generate config files following parallelism strategy<ins>: <br>\n",
    "\n",
    "(a)\tSingle GPU. (Already Provided) <br>\n",
    "(b)\tTensor Parallelism (TP=2) on 2 GPUs. (Already Provided) <br>\n",
    "(c)\tPipeline Parallelism (PP=2) on 2 GPUs. <br>\n",
    "(d)\tData Parallelism (DP=2) on 2 GPUs. <br>\n",
    "(e)\tTensor Parallelism (TP=2) + activation recomputation <br>\n",
    "(f)\tPipeline Parallelism (PP=2) + activation recomputation <br>\n",
    "(g)\tData Parallelism (DP=2) + activation recomputation <br>\n",
    "\n",
    "The degree of data parallelism is not explicitly specified but is automatically inferred as follows: <br>\n",
    "        DP = num_processes / (PP * TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "551a2552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:254: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:254: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:265: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:265: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:325: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:360: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:325: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:360: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "Initializing Megatron-LM\n",
      "using world size: 2, data-parallel size: 1, context-parallel size: 1 tensor-model-parallel size: 2, pipeline-model-parallel size: 1 \n",
      "WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  add_bias_linear ................................. True\n",
      "  add_position_embedding .......................... True\n",
      "  add_qkv_bias .................................... False\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_layernorm_1p .............................. False\n",
      "  apply_query_key_layer_scaling ................... False\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  apply_rope_fusion ............................... True\n",
      "  async_tensor_model_parallel_allreduce ........... True\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  barrier_with_L1_time ............................ True\n",
      "  bert_binary_head ................................ True\n",
      "  bert_embedder_type .............................. megatron\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  bias_swiglu_fusion .............................. True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  check_for_nan_in_loss_and_grad .................. True\n",
      "  classes_fraction ................................ 1.0\n",
      "  clip_grad ....................................... 1.0\n",
      "  clone_scatter_output_in_embedding ............... True\n",
      "  consumed_samples ................................ None\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  context_parallel_size ........................... 1\n",
      "  data_cache_path ................................. None\n",
      "  data_parallel_random_init ....................... False\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... None\n",
      "  data_per_class_fraction ......................... 1.0\n",
      "  data_sharding ................................... True\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ torch\n",
      "  decoder_num_layers .............................. None\n",
      "  decoder_seq_length .............................. None\n",
      "  delay_grad_reduce ............................... True\n",
      "  delay_param_gather .............................. False\n",
      "  dino_bottleneck_size ............................ 256\n",
      "  dino_freeze_last_layer .......................... 1\n",
      "  dino_head_hidden_size ........................... 2048\n",
      "  dino_local_crops_number ......................... 10\n",
      "  dino_local_img_size ............................. 96\n",
      "  dino_norm_last_layer ............................ False\n",
      "  dino_teacher_temp ............................... 0.07\n",
      "  dino_warmup_teacher_temp ........................ 0.04\n",
      "  dino_warmup_teacher_temp_epochs ................. 30\n",
      "  distribute_saved_activations .................... False\n",
      "  distributed_backend ............................. nccl\n",
      "  distributed_timeout_minutes ..................... 10\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_one_logger ............................... False\n",
      "  encoder_num_layers .............................. 2\n",
      "  encoder_seq_length .............................. 2048\n",
      "  end_weight_decay ................................ 0.01\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 100\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... None\n",
      "  exit_on_missing_checkpoint ...................... False\n",
      "  exit_signal_handler ............................. False\n",
      "  expert_model_parallel_size ...................... 1\n",
      "  ffn_hidden_size ................................. 10240\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  fp8 ............................................. None\n",
      "  fp8_amax_compute_algo ........................... most_recent\n",
      "  fp8_amax_history_len ............................ 1\n",
      "  fp8_interval .................................... 1\n",
      "  fp8_margin ...................................... 0\n",
      "  fp8_wgrad ....................................... True\n",
      "  global_batch_size ............................... 4\n",
      "  gradient_accumulation_fusion .................... True\n",
      "  group_query_attention ........................... False\n",
      "  head_lr_mult .................................... 1.0\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2560\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_h ........................................... 224\n",
      "  img_w ........................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  inference_batch_times_seqlen_threshold .......... 512\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  iter_per_epoch .................................. 1250\n",
      "  kv_channels ..................................... 80\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ None\n",
      "  local_rank ...................................... None\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 100\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_progress .................................... False\n",
      "  log_throughput .................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  log_world_size_to_tensorboard ................... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 5e-05\n",
      "  lr_decay_iters .................................. None\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. linear\n",
      "  lr_mult ......................................... 1.0\n",
      "  lr_warmup_fraction .............................. None\n",
      "  lr_warmup_init .................................. 0.0\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  manual_gc ....................................... False\n",
      "  manual_gc_eval .................................. True\n",
      "  manual_gc_interval .............................. 0\n",
      "  mask_factor ..................................... 1.0\n",
      "  mask_prob ....................................... 0.15\n",
      "  mask_type ....................................... random\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 2048\n",
      "  max_tokens_to_oom ............................... 12000\n",
      "  megatron_dataset_flag ........................... False\n",
      "  merge_file ...................................... /home/hice1/jho88/lab3B/src/merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 0.0\n",
      "  mock_data ....................................... False\n",
      "  model_return_dict ............................... True\n",
      "  model_type_name ................................. gpt\n",
      "  moe_aux_loss_coeff .............................. 0.0\n",
      "  moe_grouped_gemm ................................ False\n",
      "  moe_input_jitter_eps ............................ None\n",
      "  moe_router_load_balancing_type .................. aux_loss\n",
      "  moe_router_topk ................................. 2\n",
      "  moe_token_dropping .............................. False\n",
      "  moe_z_loss_coeff ................................ None\n",
      "  nccl_communicator_config_path ................... None\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_persist_layer_norm ........................... False\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  no_wd_decay_cond ................................ None\n",
      "  norm_epsilon .................................... 1e-05\n",
      "  normalization ................................... LayerNorm\n",
      "  num_attention_heads ............................. 32\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_experts ..................................... None\n",
      "  num_layers ...................................... 2\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_micro_batches ............................... 1\n",
      "  num_query_groups ................................ 1\n",
      "  num_workers ..................................... 2\n",
      "  one_logger_entity ............................... hwinf_dcm\n",
      "  one_logger_project .............................. e2e-tracking\n",
      "  one_logger_run_name ............................. None\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  orig_vocab_size ................................. 50257\n",
      "  output_bert_embeddings .......................... False\n",
      "  overlap_grad_reduce ............................. False\n",
      "  overlap_p2p_comm ................................ False\n",
      "  overlap_param_gather ............................ False\n",
      "  override_opt_param_scheduler .................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  perform_initialization .......................... True\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  position_embedding_type ......................... learned_absolute\n",
      "  pretraining_flag ................................ True\n",
      "  profile ......................................... False\n",
      "  profile_ranks ................................... [0]\n",
      "  profile_step_end ................................ 12\n",
      "  profile_step_start .............................. 10\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  recompute_granularity ........................... None\n",
      "  recompute_method ................................ None\n",
      "  recompute_num_layers ............................ None\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  retro_add_retriever ............................. False\n",
      "  retro_attention_gate ............................ 1\n",
      "  retro_cyclic_train_iters ........................ None\n",
      "  retro_encoder_attention_dropout ................. 0.1\n",
      "  retro_encoder_hidden_dropout .................... 0.1\n",
      "  retro_encoder_layers ............................ 2\n",
      "  retro_num_neighbors ............................. 2\n",
      "  retro_num_retrieved_chunks ...................... 2\n",
      "  retro_return_doc_ids ............................ False\n",
      "  retro_verify_neighbor_count ..................... True\n",
      "  retro_workdir ................................... None\n",
      "  return_logits ................................... False\n",
      "  rotary_interleaved .............................. False\n",
      "  rotary_percent .................................. 1.0\n",
      "  rotary_seq_len_interpolation_factor ............. None\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ None\n",
      "  save_interval ................................... None\n",
      "  scale_lr_cond ................................... None\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 2048\n",
      "  sequence_parallel ............................... False\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  skip_train ...................................... False\n",
      "  spec ............................................ None\n",
      "  split ........................................... 969, 30, 1\n",
      "  squared_relu .................................... False\n",
      "  standalone_embedding_stage ...................... False\n",
      "  start_weight_decay .............................. 0.01\n",
      "  swiglu .......................................... False\n",
      "  swin_backbone_type .............................. tiny\n",
      "  tensor_model_parallel_size ...................... 2\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  test_data_path .................................. None\n",
      "  timing_log_level ................................ 0\n",
      "  timing_log_option ............................... minmax\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_model ................................. None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  tp_comm_bulk_dgrad .............................. True\n",
      "  tp_comm_bulk_wgrad .............................. True\n",
      "  tp_comm_overlap ................................. False\n",
      "  tp_comm_overlap_cfg ............................. None\n",
      "  tp_comm_split_ag ................................ True\n",
      "  tp_comm_split_rs ................................ True\n",
      "  train_data_path ................................. None\n",
      "  train_iters ..................................... 1\n",
      "  train_samples ................................... None\n",
      "  transformer_impl ................................ local\n",
      "  transformer_pipeline_model_parallel_size ........ 1\n",
      "  untie_embeddings_and_output_weights ............. False\n",
      "  use_checkpoint_args ............................. False\n",
      "  use_checkpoint_opt_param_scheduler .............. False\n",
      "  use_cpu_initialization .......................... None\n",
      "  use_distributed_optimizer ....................... False\n",
      "  use_flash_attn .................................. False\n",
      "  use_mcore_models ................................ False\n",
      "  use_one_sent_docs ............................... False\n",
      "  use_ring_exchange_p2p ........................... False\n",
      "  use_rotary_position_embeddings .................. False\n",
      "  valid_data_path ................................. None\n",
      "  variable_seq_lengths ............................ False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vision_backbone_type ............................ vit\n",
      "  vision_pretraining .............................. False\n",
      "  vision_pretraining_type ......................... classify\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/hice1/jho88/lab3B/src/vocab.json\n",
      "  vocab_size ...................................... None\n",
      "  wandb_exp_name .................................. \n",
      "  wandb_project ................................... \n",
      "  wandb_save_dir .................................. \n",
      "  weight_decay .................................... 0.01\n",
      "  weight_decay_incr_style ......................... constant\n",
      "  world_size ...................................... 2\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      "[rank1]:[W227 20:58:53.989856684 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      " > padded vocab (size: 50257) with 175 dummy tokens (new size: 50432)\n",
      "torch distributed is already initialized, skipping initialization ...\n",
      "> initialized tensor model parallel with size 2\n",
      "> initialized pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> compiling dataset index builder ...\n",
      "make: Entering directory '/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/datasets'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/datasets'\n",
      ">>> done with dataset index builder. Compilation time: 0.050 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "[rank0]:[W227 20:58:53.099725735 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 0.887 seconds\n",
      "[rank1]:[W227 20:58:54.960780767 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[rank0]:[W227 20:58:54.960805952 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "Building gpt model in the pre-training mode.\n",
      "The Megatron LM model weights are initialized at random in `accelerator.prepare`. Please use `accelerator.load_checkpoint` to load a pre-trained checkpoint matching the distributed setup.\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 148492800\n",
      "> learning rate decay style: linear\n",
      "Preparing model optimizer scheduler\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 148492800\n",
      "Preparing dataloader\n",
      "[rank1]:[W227 20:59:12.023603769 CPUAllocator.cpp:245] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event\n",
      "[rank0]:[W227 20:59:12.023619754 CPUAllocator.cpp:245] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event\n",
      "---------------------------------------\n",
      "----- Profiling done successfully -----\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "----- Profiling done successfully -----\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Tensor Parallel with 2 GPU\n",
    "!accelerate launch --main_process_port {find_freeport(25900)} --config_file \"megatron_configs/tensor_parallel.yaml\"  src/prof.py --model_name \"model_configs/gpt3_27_2_layer.json\" --total_batch_size \"4\" --logdir \"prof_log/tensor_parallel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2e2b1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:254: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:254: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:265: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:265: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:325: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:325: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:360: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:360: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "using world size: 2, data-parallel size: 2, context-parallel size: 1 tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  add_bias_linear ................................. True\n",
      "  add_position_embedding .......................... True\n",
      "  add_qkv_bias .................................... False\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_layernorm_1p .............................. False\n",
      "  apply_query_key_layer_scaling ................... False\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  apply_rope_fusion ............................... True\n",
      "  async_tensor_model_parallel_allreduce ........... True\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  barrier_with_L1_time ............................ True\n",
      "  bert_binary_head ................................ True\n",
      "  bert_embedder_type .............................. megatron\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  bias_swiglu_fusion .............................. True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  check_for_nan_in_loss_and_grad .................. True\n",
      "  classes_fraction ................................ 1.0\n",
      "  clip_grad ....................................... 1.0\n",
      "  clone_scatter_output_in_embedding ............... True\n",
      "  consumed_samples ................................ None\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  context_parallel_size ........................... 1\n",
      "  data_cache_path ................................. None\n",
      "  data_parallel_random_init ....................... False\n",
      "  data_parallel_size .............................. 2\n",
      "  data_path ....................................... None\n",
      "  data_per_class_fraction ......................... 1.0\n",
      "  data_sharding ................................... True\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ torch\n",
      "  decoder_num_layers .............................. None\n",
      "  decoder_seq_length .............................. None\n",
      "  delay_grad_reduce ............................... True\n",
      "  delay_param_gather .............................. False\n",
      "  dino_bottleneck_size ............................ 256\n",
      "  dino_freeze_last_layer .......................... 1\n",
      "  dino_head_hidden_size ........................... 2048\n",
      "  dino_local_crops_number ......................... 10\n",
      "  dino_local_img_size ............................. 96\n",
      "  dino_norm_last_layer ............................ False\n",
      "  dino_teacher_temp ............................... 0.07\n",
      "  dino_warmup_teacher_temp ........................ 0.04\n",
      "  dino_warmup_teacher_temp_epochs ................. 30\n",
      "  distribute_saved_activations .................... False\n",
      "  distributed_backend ............................. nccl\n",
      "  distributed_timeout_minutes ..................... 10\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_one_logger ............................... False\n",
      "  encoder_num_layers .............................. 2\n",
      "  encoder_seq_length .............................. 2048\n",
      "  end_weight_decay ................................ 0.01\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 100\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... None\n",
      "  exit_on_missing_checkpoint ...................... False\n",
      "  exit_signal_handler ............................. False\n",
      "  expert_model_parallel_size ...................... 1\n",
      "  ffn_hidden_size ................................. 10240\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  fp8 ............................................. None\n",
      "  fp8_amax_compute_algo ........................... most_recent\n",
      "  fp8_amax_history_len ............................ 1\n",
      "  fp8_interval .................................... 1\n",
      "  fp8_margin ...................................... 0\n",
      "  fp8_wgrad ....................................... True\n",
      "  global_batch_size ............................... 4\n",
      "  gradient_accumulation_fusion .................... True\n",
      "  group_query_attention ........................... False\n",
      "  head_lr_mult .................................... 1.0\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2560\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_h ........................................... 224\n",
      "  img_w ........................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  inference_batch_times_seqlen_threshold .......... 512\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  iter_per_epoch .................................. 1250\n",
      "  kv_channels ..................................... 80\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ None\n",
      "  local_rank ...................................... None\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 100\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_progress .................................... False\n",
      "  log_throughput .................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  log_world_size_to_tensorboard ................... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 5e-05\n",
      "  lr_decay_iters .................................. None\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. linear\n",
      "  lr_mult ......................................... 1.0\n",
      "  lr_warmup_fraction .............................. None\n",
      "  lr_warmup_init .................................. 0.0\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  manual_gc ....................................... False\n",
      "  manual_gc_eval .................................. True\n",
      "  manual_gc_interval .............................. 0\n",
      "  mask_factor ..................................... 1.0\n",
      "  mask_prob ....................................... 0.15\n",
      "  mask_type ....................................... random\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 2048\n",
      "  max_tokens_to_oom ............................... 12000\n",
      "  megatron_dataset_flag ........................... False\n",
      "  merge_file ...................................... /home/hice1/jho88/lab3B/src/merges.txt\n",
      "  micro_batch_size ................................ 2\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 0.0\n",
      "  mock_data ....................................... False\n",
      "  model_return_dict ............................... True\n",
      "  model_type_name ................................. gpt\n",
      "  moe_aux_loss_coeff .............................. 0.0\n",
      "  moe_grouped_gemm ................................ False\n",
      "  moe_input_jitter_eps ............................ None\n",
      "  moe_router_load_balancing_type .................. aux_loss\n",
      "  moe_router_topk ................................. 2\n",
      "  moe_token_dropping .............................. False\n",
      "  moe_z_loss_coeff ................................ None\n",
      "  nccl_communicator_config_path ................... None\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_persist_layer_norm ........................... False\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  no_wd_decay_cond ................................ None\n",
      "  norm_epsilon .................................... 1e-05\n",
      "  normalization ................................... LayerNorm\n",
      "  num_attention_heads ............................. 32\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_experts ..................................... None\n",
      "  num_layers ...................................... 2\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_micro_batches ............................... 1\n",
      "  num_query_groups ................................ 1\n",
      "  num_workers ..................................... 2\n",
      "  one_logger_entity ............................... hwinf_dcm\n",
      "  one_logger_project .............................. e2e-tracking\n",
      "  one_logger_run_name ............................. None\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  orig_vocab_size ................................. 50257\n",
      "  output_bert_embeddings .......................... False\n",
      "  overlap_grad_reduce ............................. False\n",
      "  overlap_p2p_comm ................................ False\n",
      "  overlap_param_gather ............................ False\n",
      "  override_opt_param_scheduler .................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  perform_initialization .......................... True\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  position_embedding_type ......................... learned_absolute\n",
      "  pretraining_flag ................................ True\n",
      "  profile ......................................... False\n",
      "  profile_ranks ................................... [0]\n",
      "  profile_step_end ................................ 12\n",
      "  profile_step_start .............................. 10\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  recompute_granularity ........................... None\n",
      "  recompute_method ................................ None\n",
      "  recompute_num_layers ............................ None\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  retro_add_retriever ............................. False\n",
      "  retro_attention_gate ............................ 1\n",
      "  retro_cyclic_train_iters ........................ None\n",
      "  retro_encoder_attention_dropout ................. 0.1\n",
      "  retro_encoder_hidden_dropout .................... 0.1\n",
      "  retro_encoder_layers ............................ 2\n",
      "  retro_num_neighbors ............................. 2\n",
      "  retro_num_retrieved_chunks ...................... 2\n",
      "  retro_return_doc_ids ............................ False\n",
      "  retro_verify_neighbor_count ..................... True\n",
      "  retro_workdir ................................... None\n",
      "  return_logits ................................... False\n",
      "  rotary_interleaved .............................. False\n",
      "  rotary_percent .................................. 1.0\n",
      "  rotary_seq_len_interpolation_factor ............. None\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ None\n",
      "  save_interval ................................... None\n",
      "  scale_lr_cond ................................... None\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 2048\n",
      "  sequence_parallel ............................... False\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  skip_train ...................................... False\n",
      "  spec ............................................ None\n",
      "  split ........................................... 969, 30, 1\n",
      "  squared_relu .................................... False\n",
      "  standalone_embedding_stage ...................... False\n",
      "  start_weight_decay .............................. 0.01\n",
      "  swiglu .......................................... False\n",
      "  swin_backbone_type .............................. tiny\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  test_data_path .................................. None\n",
      "  timing_log_level ................................ 0\n",
      "  timing_log_option ............................... minmax\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_model ................................. None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  tp_comm_bulk_dgrad .............................. True\n",
      "  tp_comm_bulk_wgrad .............................. True\n",
      "  tp_comm_overlap ................................. False\n",
      "  tp_comm_overlap_cfg ............................. None\n",
      "  tp_comm_split_ag ................................ True\n",
      "  tp_comm_split_rs ................................ True\n",
      "  train_data_path ................................. None\n",
      "  train_iters ..................................... 1\n",
      "  train_samples ................................... None\n",
      "  transformer_impl ................................ local\n",
      "  transformer_pipeline_model_parallel_size ........ 1\n",
      "  untie_embeddings_and_output_weights ............. False\n",
      "  use_checkpoint_args ............................. False\n",
      "  use_checkpoint_opt_param_scheduler .............. False\n",
      "  use_cpu_initialization .......................... None\n",
      "  use_distributed_optimizer ....................... False\n",
      "  use_flash_attn .................................. False\n",
      "  use_mcore_models ................................ False\n",
      "  use_one_sent_docs ............................... False\n",
      "  use_ring_exchange_p2p ........................... False\n",
      "  use_rotary_position_embeddings .................. False\n",
      "  valid_data_path ................................. None\n",
      "  variable_seq_lengths ............................ False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vision_backbone_type ............................ vit\n",
      "  vision_pretraining .............................. False\n",
      "  vision_pretraining_type ......................... classify\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/hice1/jho88/lab3B/src/vocab.json\n",
      "  vocab_size ...................................... None\n",
      "  wandb_exp_name .................................. \n",
      "  wandb_project ................................... \n",
      "  wandb_save_dir .................................. \n",
      "  weight_decay .................................... 0.01\n",
      "  weight_decay_incr_style ......................... constant\n",
      "  world_size ...................................... 2\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      "Initializing Megatron-LM\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "torch distributed is already initialized, skipping initialization ...\n",
      "> initialized tensor model parallel with size 1\n",
      "> initialized pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "[rank1]:[W227 23:00:20.733332243 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "> compiling dataset index builder ...\n",
      "make: Entering directory '/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/datasets'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/datasets'\n",
      ">>> done with dataset index builder. Compilation time: 0.058 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "[rank0]:[W227 23:00:20.796072539 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 0.854 seconds\n",
      "[rank1]:[W227 23:00:21.645122407 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[rank0]:[W227 23:00:21.645137371 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "Building gpt model in the pre-training mode.\n",
      "The Megatron LM model weights are initialized at random in `accelerator.prepare`. Please use `accelerator.load_checkpoint` to load a pre-trained checkpoint matching the distributed setup.\n",
      "Preparing model optimizer scheduler\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 291379200\n",
      "> learning rate decay style: linear\n",
      "Preparing dataloader\n",
      "[rank0]:[W227 23:00:31.518289089 CPUAllocator.cpp:245] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event\n",
      "[rank1]:[W227 23:00:31.518302504 CPUAllocator.cpp:245] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event\n",
      "---------------------------------------\n",
      "----- Profiling done successfully -----\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "----- Profiling done successfully -----\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Data Parallel with 2 GPU\n",
    "!accelerate launch --main_process_port {find_freeport(25900)} --config_file \"megatron_configs/data_parallel.yaml\"  src/prof.py --model_name \"model_configs/gpt3_27_2_layer.json\" --total_batch_size \"4\" --logdir \"prof_log/data_parallel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e32ee04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:254: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:254: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:265: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:265: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:325: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:360: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:325: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:360: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "Initializing Megatron-LM\n",
      "using world size: 2, data-parallel size: 1, context-parallel size: 1 tensor-model-parallel size: 1, pipeline-model-parallel size: 2 \n",
      "WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  add_bias_linear ................................. True\n",
      "  add_position_embedding .......................... True\n",
      "  add_qkv_bias .................................... False\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_layernorm_1p .............................. False\n",
      "  apply_query_key_layer_scaling ................... False\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  apply_rope_fusion ............................... True\n",
      "  async_tensor_model_parallel_allreduce ........... True\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  barrier_with_L1_time ............................ True\n",
      "  bert_binary_head ................................ True\n",
      "  bert_embedder_type .............................. megatron\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  bias_swiglu_fusion .............................. True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  check_for_nan_in_loss_and_grad .................. True\n",
      "  classes_fraction ................................ 1.0\n",
      "  clip_grad ....................................... 1.0\n",
      "  clone_scatter_output_in_embedding ............... True\n",
      "  consumed_samples ................................ None\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  context_parallel_size ........................... 1\n",
      "  data_cache_path ................................. None\n",
      "  data_parallel_random_init ....................... False\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... None\n",
      "  data_per_class_fraction ......................... 1.0\n",
      "  data_sharding ................................... True\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_num_layers .............................. None\n",
      "  decoder_seq_length .............................. None\n",
      "  delay_grad_reduce ............................... True\n",
      "  delay_param_gather .............................. False\n",
      "  dino_bottleneck_size ............................ 256\n",
      "  dino_freeze_last_layer .......................... 1\n",
      "  dino_head_hidden_size ........................... 2048\n",
      "  dino_local_crops_number ......................... 10\n",
      "  dino_local_img_size ............................. 96\n",
      "  dino_norm_last_layer ............................ False\n",
      "  dino_teacher_temp ............................... 0.07\n",
      "  dino_warmup_teacher_temp ........................ 0.04\n",
      "  dino_warmup_teacher_temp_epochs ................. 30\n",
      "  distribute_saved_activations .................... False\n",
      "  distributed_backend ............................. nccl\n",
      "  distributed_timeout_minutes ..................... 10\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_one_logger ............................... False\n",
      "  encoder_num_layers .............................. 2\n",
      "  encoder_seq_length .............................. 2048\n",
      "  end_weight_decay ................................ 0.01\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 100\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... None\n",
      "  exit_on_missing_checkpoint ...................... False\n",
      "  exit_signal_handler ............................. False\n",
      "  expert_model_parallel_size ...................... 1\n",
      "  ffn_hidden_size ................................. 10240\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  fp8 ............................................. None\n",
      "  fp8_amax_compute_algo ........................... most_recent\n",
      "  fp8_amax_history_len ............................ 1\n",
      "  fp8_interval .................................... 1\n",
      "  fp8_margin ...................................... 0\n",
      "  fp8_wgrad ....................................... True\n",
      "  global_batch_size ............................... 4\n",
      "  gradient_accumulation_fusion .................... True\n",
      "  group_query_attention ........................... False\n",
      "  head_lr_mult .................................... 1.0\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2560\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_h ........................................... 224\n",
      "  img_w ........................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  inference_batch_times_seqlen_threshold .......... 512\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  iter_per_epoch .................................. 1250\n",
      "  kv_channels ..................................... 80\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ None\n",
      "  local_rank ...................................... None\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 100\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_progress .................................... False\n",
      "  log_throughput .................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  log_world_size_to_tensorboard ................... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 5e-05\n",
      "  lr_decay_iters .................................. None\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. linear\n",
      "  lr_mult ......................................... 1.0\n",
      "  lr_warmup_fraction .............................. None\n",
      "  lr_warmup_init .................................. 0.0\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  manual_gc ....................................... False\n",
      "  manual_gc_eval .................................. True\n",
      "  manual_gc_interval .............................. 0\n",
      "  mask_factor ..................................... 1.0\n",
      "  mask_prob ....................................... 0.15\n",
      "  mask_type ....................................... random\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 2048\n",
      "  max_tokens_to_oom ............................... 12000\n",
      "  megatron_dataset_flag ........................... False\n",
      "  merge_file ...................................... /home/hice1/jho88/lab3B/src/merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 0.0\n",
      "  mock_data ....................................... False\n",
      "  model_return_dict ............................... True\n",
      "  model_type_name ................................. gpt\n",
      "  moe_aux_loss_coeff .............................. 0.0\n",
      "  moe_grouped_gemm ................................ False\n",
      "  moe_input_jitter_eps ............................ None\n",
      "  moe_router_load_balancing_type .................. aux_loss\n",
      "  moe_router_topk ................................. 2\n",
      "  moe_token_dropping .............................. False\n",
      "  moe_z_loss_coeff ................................ None\n",
      "  nccl_communicator_config_path ................... None\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_persist_layer_norm ........................... False\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  no_wd_decay_cond ................................ None\n",
      "  norm_epsilon .................................... 1e-05\n",
      "  normalization ................................... LayerNorm\n",
      "  num_attention_heads ............................. 32\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_experts ..................................... None\n",
      "  num_layers ...................................... 2\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_micro_batches ............................... 1\n",
      "  num_query_groups ................................ 1\n",
      "  num_workers ..................................... 2\n",
      "  one_logger_entity ............................... hwinf_dcm\n",
      "  one_logger_project .............................. e2e-tracking\n",
      "  one_logger_run_name ............................. None\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  orig_vocab_size ................................. 50257\n",
      "  output_bert_embeddings .......................... False\n",
      "  overlap_grad_reduce ............................. False\n",
      "  overlap_p2p_comm ................................ False\n",
      "  overlap_param_gather ............................ False\n",
      "  override_opt_param_scheduler .................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  perform_initialization .......................... True\n",
      "  pipeline_model_parallel_size .................... 2\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  position_embedding_type ......................... learned_absolute\n",
      "  pretraining_flag ................................ True\n",
      "  profile ......................................... False\n",
      "  profile_ranks ................................... [0]\n",
      "  profile_step_end ................................ 12\n",
      "  profile_step_start .............................. 10\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  recompute_granularity ........................... None\n",
      "  recompute_method ................................ None\n",
      "  recompute_num_layers ............................ None\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  retro_add_retriever ............................. False\n",
      "  retro_attention_gate ............................ 1\n",
      "  retro_cyclic_train_iters ........................ None\n",
      "  retro_encoder_attention_dropout ................. 0.1\n",
      "  retro_encoder_hidden_dropout .................... 0.1\n",
      "  retro_encoder_layers ............................ 2\n",
      "  retro_num_neighbors ............................. 2\n",
      "  retro_num_retrieved_chunks ...................... 2\n",
      "  retro_return_doc_ids ............................ False\n",
      "  retro_verify_neighbor_count ..................... True\n",
      "  retro_workdir ................................... None\n",
      "  return_logits ................................... False\n",
      "  rotary_interleaved .............................. False\n",
      "  rotary_percent .................................. 1.0\n",
      "  rotary_seq_len_interpolation_factor ............. None\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ None\n",
      "  save_interval ................................... None\n",
      "  scale_lr_cond ................................... None\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 2048\n",
      "  sequence_parallel ............................... False\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  skip_train ...................................... False\n",
      "  spec ............................................ None\n",
      "  split ........................................... 969, 30, 1\n",
      "  squared_relu .................................... False\n",
      "  standalone_embedding_stage ...................... False\n",
      "  start_weight_decay .............................. 0.01\n",
      "  swiglu .......................................... False\n",
      "  swin_backbone_type .............................. tiny\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  test_data_path .................................. None\n",
      "  timing_log_level ................................ 0\n",
      "  timing_log_option ............................... minmax\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_model ................................. None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  tp_comm_bulk_dgrad .............................. True\n",
      "  tp_comm_bulk_wgrad .............................. True\n",
      "  tp_comm_overlap ................................. False\n",
      "  tp_comm_overlap_cfg ............................. None\n",
      "  tp_comm_split_ag ................................ True\n",
      "  tp_comm_split_rs ................................ True\n",
      "  train_data_path ................................. None\n",
      "  train_iters ..................................... 1\n",
      "  train_samples ................................... None\n",
      "  transformer_impl ................................ local\n",
      "  transformer_pipeline_model_parallel_size ........ 2\n",
      "  untie_embeddings_and_output_weights ............. False\n",
      "  use_checkpoint_args ............................. False\n",
      "  use_checkpoint_opt_param_scheduler .............. False\n",
      "  use_cpu_initialization .......................... None\n",
      "  use_distributed_optimizer ....................... False\n",
      "  use_flash_attn .................................. False\n",
      "  use_mcore_models ................................ False\n",
      "  use_one_sent_docs ............................... False\n",
      "  use_ring_exchange_p2p ........................... False\n",
      "  use_rotary_position_embeddings .................. False\n",
      "  valid_data_path ................................. None\n",
      "  variable_seq_lengths ............................ False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vision_backbone_type ............................ vit\n",
      "  vision_pretraining .............................. False\n",
      "  vision_pretraining_type ......................... classify\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/hice1/jho88/lab3B/src/vocab.json\n",
      "  vocab_size ...................................... None\n",
      "  wandb_exp_name .................................. \n",
      "  wandb_project ................................... \n",
      "  wandb_save_dir .................................. \n",
      "  weight_decay .................................... 0.01\n",
      "  weight_decay_incr_style ......................... constant\n",
      "  world_size ...................................... 2\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      "[rank1]:[W227 23:04:59.233128704 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "torch distributed is already initialized, skipping initialization ...\n",
      "> initialized tensor model parallel with size 1\n",
      "> initialized pipeline model parallel with size 2\n",
      "> setting random seeds to 1234 ...\n",
      "> compiling dataset index builder ...\n",
      "make: Entering directory '/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/datasets'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/datasets'\n",
      ">>> done with dataset index builder. Compilation time: 0.039 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "[rank0]:[W227 23:05:00.312141833 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 0.979 seconds\n",
      "[rank1]:[W227 23:05:01.267263253 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[rank0]:[W227 23:05:01.267280268 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "Building gpt model in the pre-training mode.\n",
      "The Megatron LM model weights are initialized at random in `accelerator.prepare`. Please use `accelerator.load_checkpoint` to load a pre-trained checkpoint matching the distributed setup.\n",
      "Preparing model optimizer scheduler\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 207459840\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 212697600\n",
      "Preparing dataloader\n",
      "> learning rate decay style: linear\n",
      "[rank0]:[W227 23:05:22.244738267 CPUAllocator.cpp:245] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event\n",
      "[rank1]:[W227 23:05:22.244752462 CPUAllocator.cpp:245] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event\n",
      "---------------------------------------\n",
      "----- Profiling done successfully -----\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "----- Profiling done successfully -----\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Pipeline Parallel with 2 GPU\n",
    "!accelerate launch --main_process_port {find_freeport(25900)} --config_file \"megatron_configs/pipeline_parallel.yaml\"  src/prof.py --model_name \"model_configs/gpt3_27_2_layer.json\" --total_batch_size \"4\" --logdir \"prof_log/pipeline_parallel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51d9ffe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:254: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:254: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:265: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:265: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:325: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:325: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:360: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:360: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "using world size: 2, data-parallel size: 1, context-parallel size: 1 tensor-model-parallel size: 2, pipeline-model-parallel size: 1 \n",
      "WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  add_bias_linear ................................. True\n",
      "  add_position_embedding .......................... True\n",
      "  add_qkv_bias .................................... False\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_layernorm_1p .............................. False\n",
      "  apply_query_key_layer_scaling ................... False\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  apply_rope_fusion ............................... True\n",
      "  async_tensor_model_parallel_allreduce ........... True\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  barrier_with_L1_time ............................ True\n",
      "  bert_binary_head ................................ True\n",
      "  bert_embedder_type .............................. megatron\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  bias_swiglu_fusion .............................. True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  check_for_nan_in_loss_and_grad .................. True\n",
      "  classes_fraction ................................ 1.0\n",
      "  clip_grad ....................................... 1.0\n",
      "  clone_scatter_output_in_embedding ............... True\n",
      "  consumed_samples ................................ None\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  context_parallel_size ........................... 1\n",
      "  data_cache_path ................................. None\n",
      "  data_parallel_random_init ....................... False\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... None\n",
      "  data_per_class_fraction ......................... 1.0\n",
      "  data_sharding ................................... True\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ torch\n",
      "  decoder_num_layers .............................. None\n",
      "  decoder_seq_length .............................. None\n",
      "  delay_grad_reduce ............................... True\n",
      "  delay_param_gather .............................. False\n",
      "  dino_bottleneck_size ............................ 256\n",
      "  dino_freeze_last_layer .......................... 1\n",
      "  dino_head_hidden_size ........................... 2048\n",
      "  dino_local_crops_number ......................... 10\n",
      "  dino_local_img_size ............................. 96\n",
      "  dino_norm_last_layer ............................ False\n",
      "  dino_teacher_temp ............................... 0.07\n",
      "  dino_warmup_teacher_temp ........................ 0.04\n",
      "  dino_warmup_teacher_temp_epochs ................. 30\n",
      "  distribute_saved_activations .................... False\n",
      "  distributed_backend ............................. nccl\n",
      "  distributed_timeout_minutes ..................... 10\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_one_logger ............................... False\n",
      "  encoder_num_layers .............................. 2\n",
      "  encoder_seq_length .............................. 2048\n",
      "  end_weight_decay ................................ 0.01\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 100\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... None\n",
      "  exit_on_missing_checkpoint ...................... False\n",
      "  exit_signal_handler ............................. False\n",
      "  expert_model_parallel_size ...................... 1\n",
      "  ffn_hidden_size ................................. 10240\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  fp8 ............................................. None\n",
      "  fp8_amax_compute_algo ........................... most_recent\n",
      "  fp8_amax_history_len ............................ 1\n",
      "  fp8_interval .................................... 1\n",
      "  fp8_margin ...................................... 0\n",
      "  fp8_wgrad ....................................... True\n",
      "  global_batch_size ............................... 4\n",
      "  gradient_accumulation_fusion .................... True\n",
      "  group_query_attention ........................... False\n",
      "  head_lr_mult .................................... 1.0\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2560\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_h ........................................... 224\n",
      "  img_w ........................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  inference_batch_times_seqlen_threshold .......... 512\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  iter_per_epoch .................................. 1250\n",
      "  kv_channels ..................................... 80\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ None\n",
      "  local_rank ...................................... None\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 100\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_progress .................................... False\n",
      "  log_throughput .................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  log_world_size_to_tensorboard ................... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 5e-05\n",
      "  lr_decay_iters .................................. None\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. linear\n",
      "  lr_mult ......................................... 1.0\n",
      "  lr_warmup_fraction .............................. None\n",
      "  lr_warmup_init .................................. 0.0\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  manual_gc ....................................... False\n",
      "  manual_gc_eval .................................. True\n",
      "  manual_gc_interval .............................. 0\n",
      "  mask_factor ..................................... 1.0\n",
      "  mask_prob ....................................... 0.15\n",
      "  mask_type ....................................... random\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 2048\n",
      "  max_tokens_to_oom ............................... 12000\n",
      "  megatron_dataset_flag ........................... False\n",
      "  merge_file ...................................... /home/hice1/jho88/lab3B/src/merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 0.0\n",
      "  mock_data ....................................... False\n",
      "  model_return_dict ............................... True\n",
      "  model_type_name ................................. gpt\n",
      "  moe_aux_loss_coeff .............................. 0.0\n",
      "  moe_grouped_gemm ................................ False\n",
      "  moe_input_jitter_eps ............................ None\n",
      "  moe_router_load_balancing_type .................. aux_loss\n",
      "  moe_router_topk ................................. 2\n",
      "  moe_token_dropping .............................. False\n",
      "  moe_z_loss_coeff ................................ None\n",
      "  nccl_communicator_config_path ................... None\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_persist_layer_norm ........................... False\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  no_wd_decay_cond ................................ None\n",
      "  norm_epsilon .................................... 1e-05\n",
      "  normalization ................................... LayerNorm\n",
      "  num_attention_heads ............................. 32\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_experts ..................................... None\n",
      "  num_layers ...................................... 2\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_micro_batches ............................... 1\n",
      "  num_query_groups ................................ 1\n",
      "  num_workers ..................................... 2\n",
      "  one_logger_entity ............................... hwinf_dcm\n",
      "  one_logger_project .............................. e2e-tracking\n",
      "  one_logger_run_name ............................. None\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  orig_vocab_size ................................. 50257\n",
      "  output_bert_embeddings .......................... False\n",
      "  overlap_grad_reduce ............................. False\n",
      "  overlap_p2p_comm ................................ False\n",
      "  overlap_param_gather ............................ False\n",
      "  override_opt_param_scheduler .................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  perform_initialization .......................... True\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  position_embedding_type ......................... learned_absolute\n",
      "  pretraining_flag ................................ True\n",
      "  profile ......................................... False\n",
      "  profile_ranks ................................... [0]\n",
      "  profile_step_end ................................ 12\n",
      "  profile_step_start .............................. 10\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  recompute_granularity ........................... selective\n",
      "  recompute_method ................................ None\n",
      "  recompute_num_layers ............................ None\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  retro_add_retriever ............................. False\n",
      "  retro_attention_gate ............................ 1\n",
      "  retro_cyclic_train_iters ........................ None\n",
      "  retro_encoder_attention_dropout ................. 0.1\n",
      "  retro_encoder_hidden_dropout .................... 0.1\n",
      "  retro_encoder_layers ............................ 2\n",
      "  retro_num_neighbors ............................. 2\n",
      "  retro_num_retrieved_chunks ...................... 2\n",
      "  retro_return_doc_ids ............................ False\n",
      "  retro_verify_neighbor_count ..................... True\n",
      "  retro_workdir ................................... None\n",
      "  return_logits ................................... False\n",
      "  rotary_interleaved .............................. False\n",
      "  rotary_percent .................................. 1.0\n",
      "  rotary_seq_len_interpolation_factor ............. None\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ None\n",
      "  save_interval ................................... None\n",
      "  scale_lr_cond ................................... None\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 2048\n",
      "  sequence_parallel ............................... False\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  skip_train ...................................... False\n",
      "  spec ............................................ None\n",
      "  split ........................................... 969, 30, 1\n",
      "  squared_relu .................................... False\n",
      "  standalone_embedding_stage ...................... False\n",
      "  start_weight_decay .............................. 0.01\n",
      "  swiglu .......................................... False\n",
      "  swin_backbone_type .............................. tiny\n",
      "  tensor_model_parallel_size ...................... 2\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  test_data_path .................................. None\n",
      "  timing_log_level ................................ 0\n",
      "  timing_log_option ............................... minmax\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_model ................................. None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  tp_comm_bulk_dgrad .............................. True\n",
      "  tp_comm_bulk_wgrad .............................. True\n",
      "  tp_comm_overlap ................................. False\n",
      "  tp_comm_overlap_cfg ............................. None\n",
      "  tp_comm_split_ag ................................ True\n",
      "  tp_comm_split_rs ................................ True\n",
      "  train_data_path ................................. None\n",
      "  train_iters ..................................... 1\n",
      "  train_samples ................................... None\n",
      "  transformer_impl ................................ local\n",
      "  transformer_pipeline_model_parallel_size ........ 1\n",
      "  untie_embeddings_and_output_weights ............. False\n",
      "  use_checkpoint_args ............................. False\n",
      "  use_checkpoint_opt_param_scheduler .............. False\n",
      "  use_cpu_initialization .......................... None\n",
      "  use_distributed_optimizer ....................... False\n",
      "  use_flash_attn .................................. False\n",
      "  use_mcore_models ................................ False\n",
      "  use_one_sent_docs ............................... False\n",
      "  use_ring_exchange_p2p ........................... False\n",
      "  use_rotary_position_embeddings .................. False\n",
      "  valid_data_path ................................. None\n",
      "  variable_seq_lengths ............................ False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vision_backbone_type ............................ vit\n",
      "  vision_pretraining .............................. False\n",
      "  vision_pretraining_type ......................... classify\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/hice1/jho88/lab3B/src/vocab.json\n",
      "  vocab_size ...................................... None\n",
      "  wandb_exp_name .................................. \n",
      "  wandb_project ................................... \n",
      "  wandb_save_dir .................................. \n",
      "  weight_decay .................................... 0.01\n",
      "  weight_decay_incr_style ......................... constant\n",
      "  world_size ...................................... 2\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      "Initializing Megatron-LM\n",
      " > padded vocab (size: 50257) with 175 dummy tokens (new size: 50432)\n",
      "torch distributed is already initialized, skipping initialization ...\n",
      "> initialized tensor model parallel with size 2\n",
      "> initialized pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> compiling dataset index builder ...\n",
      "make: Entering directory '/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/datasets'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/datasets'\n",
      ">>> done with dataset index builder. Compilation time: 0.041 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "[rank1]:[W227 23:14:43.113555866 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "[rank0]:[W227 23:14:43.135340215 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 0.947 seconds\n",
      "[rank1]:[W227 23:14:44.057088540 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[rank0]:[W227 23:14:44.057092940 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "Preparing model optimizer scheduler\n",
      "Building gpt model in the pre-training mode.\n",
      "The Megatron LM model weights are initialized at random in `accelerator.prepare`. Please use `accelerator.load_checkpoint` to load a pre-trained checkpoint matching the distributed setup.\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 148492800\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 148492800\n",
      "Preparing dataloader\n",
      "> learning rate decay style: linear\n",
      "[rank1]:[W227 23:14:50.103301558 CPUAllocator.cpp:245] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event\n",
      "[rank0]:[W227 23:14:50.103318731 CPUAllocator.cpp:245] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event\n",
      "---------------------------------------\n",
      "----- Profiling done successfully -----\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "----- Profiling done successfully -----\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Tensor Parallel with 2 GPU with activation recomputation\n",
    "!accelerate launch --main_process_port {find_freeport(25900)} --config_file \"megatron_configs/tensor_parallel_w_activ_recomp.yaml\"  src/prof.py --model_name \"model_configs/gpt3_27_2_layer.json\" --total_batch_size \"4\" --logdir \"prof_log/tensor_parallel_w_activ_recomp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfec7b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:254: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:265: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:325: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:360: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:254: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:265: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:325: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:360: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "using world size: 2, data-parallel size: 2, context-parallel size: 1 tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  add_bias_linear ................................. True\n",
      "  add_position_embedding .......................... True\n",
      "  add_qkv_bias .................................... False\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_layernorm_1p .............................. False\n",
      "  apply_query_key_layer_scaling ................... False\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  apply_rope_fusion ............................... True\n",
      "  async_tensor_model_parallel_allreduce ........... True\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  barrier_with_L1_time ............................ True\n",
      "  bert_binary_head ................................ True\n",
      "  bert_embedder_type .............................. megatron\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  bias_swiglu_fusion .............................. True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  check_for_nan_in_loss_and_grad .................. True\n",
      "  classes_fraction ................................ 1.0\n",
      "  clip_grad ....................................... 1.0\n",
      "  clone_scatter_output_in_embedding ............... True\n",
      "  consumed_samples ................................ None\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  context_parallel_size ........................... 1\n",
      "  data_cache_path ................................. None\n",
      "  data_parallel_random_init ....................... False\n",
      "  data_parallel_size .............................. 2\n",
      "  data_path ....................................... None\n",
      "  data_per_class_fraction ......................... 1.0\n",
      "  data_sharding ................................... True\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ torch\n",
      "  decoder_num_layers .............................. None\n",
      "  decoder_seq_length .............................. None\n",
      "  delay_grad_reduce ............................... True\n",
      "  delay_param_gather .............................. False\n",
      "  dino_bottleneck_size ............................ 256\n",
      "  dino_freeze_last_layer .......................... 1\n",
      "  dino_head_hidden_size ........................... 2048\n",
      "  dino_local_crops_number ......................... 10\n",
      "  dino_local_img_size ............................. 96\n",
      "  dino_norm_last_layer ............................ False\n",
      "  dino_teacher_temp ............................... 0.07\n",
      "  dino_warmup_teacher_temp ........................ 0.04\n",
      "  dino_warmup_teacher_temp_epochs ................. 30\n",
      "  distribute_saved_activations .................... False\n",
      "  distributed_backend ............................. nccl\n",
      "  distributed_timeout_minutes ..................... 10\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_one_logger ............................... False\n",
      "  encoder_num_layers .............................. 2\n",
      "  encoder_seq_length .............................. 2048\n",
      "  end_weight_decay ................................ 0.01\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 100\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... None\n",
      "  exit_on_missing_checkpoint ...................... False\n",
      "  exit_signal_handler ............................. False\n",
      "  expert_model_parallel_size ...................... 1\n",
      "  ffn_hidden_size ................................. 10240\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  fp8 ............................................. None\n",
      "  fp8_amax_compute_algo ........................... most_recent\n",
      "  fp8_amax_history_len ............................ 1\n",
      "  fp8_interval .................................... 1\n",
      "  fp8_margin ...................................... 0\n",
      "  fp8_wgrad ....................................... True\n",
      "  global_batch_size ............................... 4\n",
      "  gradient_accumulation_fusion .................... True\n",
      "  group_query_attention ........................... False\n",
      "  head_lr_mult .................................... 1.0\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2560\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_h ........................................... 224\n",
      "  img_w ........................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  inference_batch_times_seqlen_threshold .......... 512\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  iter_per_epoch .................................. 1250\n",
      "  kv_channels ..................................... 80\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ None\n",
      "  local_rank ...................................... None\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 100\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_progress .................................... False\n",
      "  log_throughput .................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  log_world_size_to_tensorboard ................... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 5e-05\n",
      "  lr_decay_iters .................................. None\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. linear\n",
      "  lr_mult ......................................... 1.0\n",
      "  lr_warmup_fraction .............................. None\n",
      "  lr_warmup_init .................................. 0.0\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  manual_gc ....................................... False\n",
      "  manual_gc_eval .................................. True\n",
      "  manual_gc_interval .............................. 0\n",
      "  mask_factor ..................................... 1.0\n",
      "  mask_prob ....................................... 0.15\n",
      "  mask_type ....................................... random\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 2048\n",
      "  max_tokens_to_oom ............................... 12000\n",
      "  megatron_dataset_flag ........................... False\n",
      "  merge_file ...................................... /home/hice1/jho88/lab3B/src/merges.txt\n",
      "  micro_batch_size ................................ 2\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 0.0\n",
      "  mock_data ....................................... False\n",
      "  model_return_dict ............................... True\n",
      "  model_type_name ................................. gpt\n",
      "  moe_aux_loss_coeff .............................. 0.0\n",
      "  moe_grouped_gemm ................................ False\n",
      "  moe_input_jitter_eps ............................ None\n",
      "  moe_router_load_balancing_type .................. aux_loss\n",
      "  moe_router_topk ................................. 2\n",
      "  moe_token_dropping .............................. False\n",
      "  moe_z_loss_coeff ................................ None\n",
      "  nccl_communicator_config_path ................... None\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_persist_layer_norm ........................... False\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  no_wd_decay_cond ................................ None\n",
      "  norm_epsilon .................................... 1e-05\n",
      "  normalization ................................... LayerNorm\n",
      "  num_attention_heads ............................. 32\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_experts ..................................... None\n",
      "  num_layers ...................................... 2\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_micro_batches ............................... 1\n",
      "  num_query_groups ................................ 1\n",
      "  num_workers ..................................... 2\n",
      "  one_logger_entity ............................... hwinf_dcm\n",
      "  one_logger_project .............................. e2e-tracking\n",
      "  one_logger_run_name ............................. None\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  orig_vocab_size ................................. 50257\n",
      "  output_bert_embeddings .......................... False\n",
      "  overlap_grad_reduce ............................. False\n",
      "  overlap_p2p_comm ................................ False\n",
      "  overlap_param_gather ............................ False\n",
      "  override_opt_param_scheduler .................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  perform_initialization .......................... True\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  position_embedding_type ......................... learned_absolute\n",
      "  pretraining_flag ................................ True\n",
      "  profile ......................................... False\n",
      "  profile_ranks ................................... [0]\n",
      "  profile_step_end ................................ 12\n",
      "  profile_step_start .............................. 10\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  recompute_granularity ........................... selective\n",
      "  recompute_method ................................ None\n",
      "  recompute_num_layers ............................ None\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  retro_add_retriever ............................. False\n",
      "  retro_attention_gate ............................ 1\n",
      "  retro_cyclic_train_iters ........................ None\n",
      "  retro_encoder_attention_dropout ................. 0.1\n",
      "  retro_encoder_hidden_dropout .................... 0.1\n",
      "  retro_encoder_layers ............................ 2\n",
      "  retro_num_neighbors ............................. 2\n",
      "  retro_num_retrieved_chunks ...................... 2\n",
      "  retro_return_doc_ids ............................ False\n",
      "  retro_verify_neighbor_count ..................... True\n",
      "  retro_workdir ................................... None\n",
      "  return_logits ................................... False\n",
      "  rotary_interleaved .............................. False\n",
      "  rotary_percent .................................. 1.0\n",
      "  rotary_seq_len_interpolation_factor ............. None\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ None\n",
      "  save_interval ................................... None\n",
      "  scale_lr_cond ................................... None\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 2048\n",
      "  sequence_parallel ............................... False\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  skip_train ...................................... False\n",
      "  spec ............................................ None\n",
      "  split ........................................... 969, 30, 1\n",
      "  squared_relu .................................... False\n",
      "  standalone_embedding_stage ...................... False\n",
      "  start_weight_decay .............................. 0.01\n",
      "  swiglu .......................................... False\n",
      "  swin_backbone_type .............................. tiny\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  test_data_path .................................. None\n",
      "  timing_log_level ................................ 0\n",
      "  timing_log_option ............................... minmax\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_model ................................. None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  tp_comm_bulk_dgrad .............................. True\n",
      "  tp_comm_bulk_wgrad .............................. True\n",
      "  tp_comm_overlap ................................. False\n",
      "  tp_comm_overlap_cfg ............................. None\n",
      "  tp_comm_split_ag ................................ True\n",
      "  tp_comm_split_rs ................................ True\n",
      "  train_data_path ................................. None\n",
      "  train_iters ..................................... 1\n",
      "  train_samples ................................... None\n",
      "  transformer_impl ................................ local\n",
      "  transformer_pipeline_model_parallel_size ........ 1\n",
      "  untie_embeddings_and_output_weights ............. False\n",
      "  use_checkpoint_args ............................. False\n",
      "  use_checkpoint_opt_param_scheduler .............. False\n",
      "  use_cpu_initialization .......................... None\n",
      "  use_distributed_optimizer ....................... False\n",
      "  use_flash_attn .................................. False\n",
      "  use_mcore_models ................................ False\n",
      "  use_one_sent_docs ............................... False\n",
      "  use_ring_exchange_p2p ........................... False\n",
      "  use_rotary_position_embeddings .................. False\n",
      "  valid_data_path ................................. None\n",
      "  variable_seq_lengths ............................ False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vision_backbone_type ............................ vit\n",
      "  vision_pretraining .............................. False\n",
      "  vision_pretraining_type ......................... classify\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/hice1/jho88/lab3B/src/vocab.json\n",
      "  vocab_size ...................................... None\n",
      "  wandb_exp_name .................................. \n",
      "  wandb_project ................................... \n",
      "  wandb_save_dir .................................. \n",
      "  weight_decay .................................... 0.01\n",
      "  weight_decay_incr_style ......................... constant\n",
      "  world_size ...................................... 2\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      "Initializing Megatron-LM\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "torch distributed is already initialized, skipping initialization ...\n",
      "> initialized tensor model parallel with size 1\n",
      "> initialized pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "[rank1]:[W227 23:15:08.079161034 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "> compiling dataset index builder ...\n",
      "make: Entering directory '/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/datasets'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/datasets'\n",
      ">>> done with dataset index builder. Compilation time: 0.051 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "[rank0]:[W227 23:15:08.133641443 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 0.868 seconds\n",
      "[rank1]:[W227 23:15:09.998850421 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[rank0]:[W227 23:15:09.998867123 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "Preparing model optimizer scheduler\n",
      "Building gpt model in the pre-training mode.\n",
      "The Megatron LM model weights are initialized at random in `accelerator.prepare`. Please use `accelerator.load_checkpoint` to load a pre-trained checkpoint matching the distributed setup.\n",
      "Preparing dataloader\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 291379200\n",
      "> learning rate decay style: linear\n",
      "[rank0]:[W227 23:15:19.405174836 CPUAllocator.cpp:245] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event\n",
      "[rank1]:[W227 23:15:19.405174289 CPUAllocator.cpp:245] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event\n",
      "---------------------------------------\n",
      "----- Profiling done successfully -----\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "----- Profiling done successfully -----\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Data Parallel with 2 GPU with activation recomputation\n",
    "!accelerate launch --main_process_port {find_freeport(25900)} --config_file \"megatron_configs/data_parallel_w_activ_recomp.yaml\"  src/prof.py --model_name \"model_configs/gpt3_27_2_layer.json\" --total_batch_size \"4\" --logdir \"prof_log/data_parallel_w_activ_recomp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ab7a03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:254: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:265: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:325: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:360: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:254: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:265: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:325: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/tensor_parallel/layers.py:360: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "using world size: 2, data-parallel size: 1, context-parallel size: 1 tensor-model-parallel size: 1, pipeline-model-parallel size: 2 \n",
      "WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  add_bias_linear ................................. True\n",
      "  add_position_embedding .......................... True\n",
      "  add_qkv_bias .................................... False\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_layernorm_1p .............................. False\n",
      "  apply_query_key_layer_scaling ................... False\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  apply_rope_fusion ............................... True\n",
      "  async_tensor_model_parallel_allreduce ........... True\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  barrier_with_L1_time ............................ True\n",
      "  bert_binary_head ................................ True\n",
      "  bert_embedder_type .............................. megatron\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  bias_swiglu_fusion .............................. True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  check_for_nan_in_loss_and_grad .................. True\n",
      "  classes_fraction ................................ 1.0\n",
      "  clip_grad ....................................... 1.0\n",
      "  clone_scatter_output_in_embedding ............... True\n",
      "  consumed_samples ................................ None\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  context_parallel_size ........................... 1\n",
      "  data_cache_path ................................. None\n",
      "  data_parallel_random_init ....................... False\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... None\n",
      "  data_per_class_fraction ......................... 1.0\n",
      "  data_sharding ................................... True\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_num_layers .............................. None\n",
      "  decoder_seq_length .............................. None\n",
      "  delay_grad_reduce ............................... True\n",
      "  delay_param_gather .............................. False\n",
      "  dino_bottleneck_size ............................ 256\n",
      "  dino_freeze_last_layer .......................... 1\n",
      "  dino_head_hidden_size ........................... 2048\n",
      "  dino_local_crops_number ......................... 10\n",
      "  dino_local_img_size ............................. 96\n",
      "  dino_norm_last_layer ............................ False\n",
      "  dino_teacher_temp ............................... 0.07\n",
      "  dino_warmup_teacher_temp ........................ 0.04\n",
      "  dino_warmup_teacher_temp_epochs ................. 30\n",
      "  distribute_saved_activations .................... False\n",
      "  distributed_backend ............................. nccl\n",
      "  distributed_timeout_minutes ..................... 10\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_one_logger ............................... False\n",
      "  encoder_num_layers .............................. 2\n",
      "  encoder_seq_length .............................. 2048\n",
      "  end_weight_decay ................................ 0.01\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 100\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... None\n",
      "  exit_on_missing_checkpoint ...................... False\n",
      "  exit_signal_handler ............................. False\n",
      "  expert_model_parallel_size ...................... 1\n",
      "  ffn_hidden_size ................................. 10240\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  fp8 ............................................. None\n",
      "  fp8_amax_compute_algo ........................... most_recent\n",
      "  fp8_amax_history_len ............................ 1\n",
      "  fp8_interval .................................... 1\n",
      "  fp8_margin ...................................... 0\n",
      "  fp8_wgrad ....................................... True\n",
      "  global_batch_size ............................... 4\n",
      "  gradient_accumulation_fusion .................... True\n",
      "  group_query_attention ........................... False\n",
      "  head_lr_mult .................................... 1.0\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2560\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_h ........................................... 224\n",
      "  img_w ........................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  inference_batch_times_seqlen_threshold .......... 512\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  iter_per_epoch .................................. 1250\n",
      "  kv_channels ..................................... 80\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ None\n",
      "  local_rank ...................................... None\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 100\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_progress .................................... False\n",
      "  log_throughput .................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  log_world_size_to_tensorboard ................... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 5e-05\n",
      "  lr_decay_iters .................................. None\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. linear\n",
      "  lr_mult ......................................... 1.0\n",
      "  lr_warmup_fraction .............................. None\n",
      "  lr_warmup_init .................................. 0.0\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  manual_gc ....................................... False\n",
      "  manual_gc_eval .................................. True\n",
      "  manual_gc_interval .............................. 0\n",
      "  mask_factor ..................................... 1.0\n",
      "  mask_prob ....................................... 0.15\n",
      "  mask_type ....................................... random\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 2048\n",
      "  max_tokens_to_oom ............................... 12000\n",
      "  megatron_dataset_flag ........................... False\n",
      "  merge_file ...................................... /home/hice1/jho88/lab3B/src/merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 0.0\n",
      "  mock_data ....................................... False\n",
      "  model_return_dict ............................... True\n",
      "  model_type_name ................................. gpt\n",
      "  moe_aux_loss_coeff .............................. 0.0\n",
      "  moe_grouped_gemm ................................ False\n",
      "  moe_input_jitter_eps ............................ None\n",
      "  moe_router_load_balancing_type .................. aux_loss\n",
      "  moe_router_topk ................................. 2\n",
      "  moe_token_dropping .............................. False\n",
      "  moe_z_loss_coeff ................................ None\n",
      "  nccl_communicator_config_path ................... None\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_persist_layer_norm ........................... False\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  no_wd_decay_cond ................................ None\n",
      "  norm_epsilon .................................... 1e-05\n",
      "  normalization ................................... LayerNorm\n",
      "  num_attention_heads ............................. 32\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_experts ..................................... None\n",
      "  num_layers ...................................... 2\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_micro_batches ............................... 1\n",
      "  num_query_groups ................................ 1\n",
      "  num_workers ..................................... 2\n",
      "  one_logger_entity ............................... hwinf_dcm\n",
      "  one_logger_project .............................. e2e-tracking\n",
      "  one_logger_run_name ............................. None\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  orig_vocab_size ................................. 50257\n",
      "  output_bert_embeddings .......................... False\n",
      "  overlap_grad_reduce ............................. False\n",
      "  overlap_p2p_comm ................................ False\n",
      "  overlap_param_gather ............................ False\n",
      "  override_opt_param_scheduler .................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  perform_initialization .......................... True\n",
      "  pipeline_model_parallel_size .................... 2\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  position_embedding_type ......................... learned_absolute\n",
      "  pretraining_flag ................................ True\n",
      "  profile ......................................... False\n",
      "  profile_ranks ................................... [0]\n",
      "  profile_step_end ................................ 12\n",
      "  profile_step_start .............................. 10\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  recompute_granularity ........................... selective\n",
      "  recompute_method ................................ None\n",
      "  recompute_num_layers ............................ None\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  retro_add_retriever ............................. False\n",
      "  retro_attention_gate ............................ 1\n",
      "  retro_cyclic_train_iters ........................ None\n",
      "  retro_encoder_attention_dropout ................. 0.1\n",
      "  retro_encoder_hidden_dropout .................... 0.1\n",
      "  retro_encoder_layers ............................ 2\n",
      "  retro_num_neighbors ............................. 2\n",
      "  retro_num_retrieved_chunks ...................... 2\n",
      "  retro_return_doc_ids ............................ False\n",
      "  retro_verify_neighbor_count ..................... True\n",
      "  retro_workdir ................................... None\n",
      "  return_logits ................................... False\n",
      "  rotary_interleaved .............................. False\n",
      "  rotary_percent .................................. 1.0\n",
      "  rotary_seq_len_interpolation_factor ............. None\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ None\n",
      "  save_interval ................................... None\n",
      "  scale_lr_cond ................................... None\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 2048\n",
      "  sequence_parallel ............................... False\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  skip_train ...................................... False\n",
      "  spec ............................................ None\n",
      "  split ........................................... 969, 30, 1\n",
      "  squared_relu .................................... False\n",
      "  standalone_embedding_stage ...................... False\n",
      "  start_weight_decay .............................. 0.01\n",
      "  swiglu .......................................... False\n",
      "  swin_backbone_type .............................. tiny\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  test_data_path .................................. None\n",
      "  timing_log_level ................................ 0\n",
      "  timing_log_option ............................... minmax\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_model ................................. None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  tp_comm_bulk_dgrad .............................. True\n",
      "  tp_comm_bulk_wgrad .............................. True\n",
      "  tp_comm_overlap ................................. False\n",
      "  tp_comm_overlap_cfg ............................. None\n",
      "  tp_comm_split_ag ................................ True\n",
      "  tp_comm_split_rs ................................ True\n",
      "  train_data_path ................................. None\n",
      "  train_iters ..................................... 1\n",
      "  train_samples ................................... None\n",
      "  transformer_impl ................................ local\n",
      "  transformer_pipeline_model_parallel_size ........ 2\n",
      "  untie_embeddings_and_output_weights ............. False\n",
      "  use_checkpoint_args ............................. False\n",
      "  use_checkpoint_opt_param_scheduler .............. False\n",
      "  use_cpu_initialization .......................... None\n",
      "  use_distributed_optimizer ....................... False\n",
      "  use_flash_attn .................................. False\n",
      "  use_mcore_models ................................ False\n",
      "  use_one_sent_docs ............................... False\n",
      "  use_ring_exchange_p2p ........................... False\n",
      "  use_rotary_position_embeddings .................. False\n",
      "  valid_data_path ................................. None\n",
      "  variable_seq_lengths ............................ False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vision_backbone_type ............................ vit\n",
      "  vision_pretraining .............................. False\n",
      "  vision_pretraining_type ......................... classify\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/hice1/jho88/lab3B/src/vocab.json\n",
      "  vocab_size ...................................... None\n",
      "  wandb_exp_name .................................. \n",
      "  wandb_project ................................... \n",
      "  wandb_save_dir .................................. \n",
      "  weight_decay .................................... 0.01\n",
      "  weight_decay_incr_style ......................... constant\n",
      "  world_size ...................................... 2\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "torch distributed is already initialized, skipping initialization ...\n",
      "> initialized tensor model parallel with size 1\n",
      "> initialized pipeline model parallel with size 2\n",
      "> setting random seeds to 1234 ...\n",
      "> compiling dataset index builder ...\n",
      "make: Entering directory '/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/datasets'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/datasets'\n",
      ">>> done with dataset index builder. Compilation time: 0.038 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Initializing Megatron-LM\n",
      "[rank0]:[W227 23:15:42.494567297 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "[rank1]:[W227 23:15:42.550321581 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 0.924 seconds\n",
      "[rank1]:[W227 23:15:43.403860009 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[rank0]:[W227 23:15:43.403876650 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "Building gpt model in the pre-training mode.\n",
      "The Megatron LM model weights are initialized at random in `accelerator.prepare`. Please use `accelerator.load_checkpoint` to load a pre-trained checkpoint matching the distributed setup.\n",
      "Preparing model optimizer scheduler\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 212697600\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 207459840\n",
      "Preparing dataloader\n",
      "> learning rate decay style: linear\n",
      "[rank0]:[W227 23:15:51.689276196 CPUAllocator.cpp:245] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event\n",
      "[rank1]:[W227 23:15:51.689319178 CPUAllocator.cpp:245] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event\n",
      "---------------------------------------\n",
      "----- Profiling done successfully -----\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "----- Profiling done successfully -----\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Pipeline Parallel with 2 GPU with activation recomputation\n",
    "!accelerate launch --main_process_port {find_freeport(25900)} --config_file \"megatron_configs/pipeline_parallel_w_activ_recomp.yaml\"  src/prof.py --model_name \"model_configs/gpt3_27_2_layer.json\" --total_batch_size \"4\" --logdir \"prof_log/pipeline_parallel_w_activ_recomp\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec32d7bc",
   "metadata": {},
   "source": [
    "Use [TensorBoard](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html) to analyze various characteristics of different runs. Fill in the provided **Excel sheet** for all **seven configurations**. Refer to the lab instructions for the details to be filled in the Excel sheet.\n",
    "\n",
    "To open the TensorBoard session on port X, follow the steps below:\n",
    "\n",
    "(a) Press ctrl+shift+p (or cmd+shift+p for Mac) on your keyboard to open command palette \n",
    "    \n",
    "(b) Type \">open port in browser\" \n",
    "    \n",
    "(c) Select port X\n",
    "    \n",
    "If TensorBoard only shows a stale log data, restart the TensorBoard by running cells again to open the tensorboard on a different port."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4621f654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorboard will be open on port: 6006\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "freeport = find_freeport(6006)\n",
    "print(\"Tensorboard will be open on port:\", freeport) \n",
    "!tensorboard --logdir ~/lab3B/prof_log/ --port {freeport} --bind_all --reload_multifile True 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527ab504",
   "metadata": {},
   "source": [
    "## Part B-2: Training a large model\n",
    "\n",
    "Train the largest possible GPT-like model with a batch size of 4 on 2 H100 GPUs, by modifying the following parameters:  \n",
    "\n",
    "1. **Model Size**:  \n",
    "   - Edit the `\"model_configs/gpt3_27.json\"` file.  \n",
    "   - You may only modify the number of layers (`\"n_layer\": 24`).  Set the number of layers to a multiple of 24.\n",
    "\n",
    "2. **Distributed Training Configuration**:  \n",
    "   - Choose any of the six configurations from Part B-1.  \n",
    "\n",
    "If the training cell executes successfully, it will report the model's size. If the model is too large, it will result in an out-of-memory error, triggering a CUDA or NCCL error.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52d48058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Megatron-LM\n",
      "using world size: 2, data-parallel size: 1, context-parallel size: 1 tensor-model-parallel size: 1, pipeline-model-parallel size: 2 \n",
      "WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  add_bias_linear ................................. True\n",
      "  add_position_embedding .......................... True\n",
      "  add_qkv_bias .................................... False\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_layernorm_1p .............................. False\n",
      "  apply_query_key_layer_scaling ................... False\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  apply_rope_fusion ............................... True\n",
      "  async_tensor_model_parallel_allreduce ........... True\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  barrier_with_L1_time ............................ True\n",
      "  bert_binary_head ................................ True\n",
      "  bert_embedder_type .............................. megatron\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  bias_swiglu_fusion .............................. True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  check_for_nan_in_loss_and_grad .................. True\n",
      "  classes_fraction ................................ 1.0\n",
      "  clip_grad ....................................... 1.0\n",
      "  clone_scatter_output_in_embedding ............... True\n",
      "  consumed_samples ................................ None\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  context_parallel_size ........................... 1\n",
      "  data_cache_path ................................. None\n",
      "  data_parallel_random_init ....................... False\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... None\n",
      "  data_per_class_fraction ......................... 1.0\n",
      "  data_sharding ................................... True\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_num_layers .............................. None\n",
      "  decoder_seq_length .............................. None\n",
      "  delay_grad_reduce ............................... True\n",
      "  delay_param_gather .............................. False\n",
      "  dino_bottleneck_size ............................ 256\n",
      "  dino_freeze_last_layer .......................... 1\n",
      "  dino_head_hidden_size ........................... 2048\n",
      "  dino_local_crops_number ......................... 10\n",
      "  dino_local_img_size ............................. 96\n",
      "  dino_norm_last_layer ............................ False\n",
      "  dino_teacher_temp ............................... 0.07\n",
      "  dino_warmup_teacher_temp ........................ 0.04\n",
      "  dino_warmup_teacher_temp_epochs ................. 30\n",
      "  distribute_saved_activations .................... False\n",
      "  distributed_backend ............................. nccl\n",
      "  distributed_timeout_minutes ..................... 10\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_one_logger ............................... False\n",
      "  encoder_num_layers .............................. 96\n",
      "  encoder_seq_length .............................. 1024\n",
      "  end_weight_decay ................................ 0.01\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 100\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... None\n",
      "  exit_on_missing_checkpoint ...................... False\n",
      "  exit_signal_handler ............................. False\n",
      "  expert_model_parallel_size ...................... 1\n",
      "  ffn_hidden_size ................................. 10240\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  fp8 ............................................. None\n",
      "  fp8_amax_compute_algo ........................... most_recent\n",
      "  fp8_amax_history_len ............................ 1\n",
      "  fp8_interval .................................... 1\n",
      "  fp8_margin ...................................... 0\n",
      "  fp8_wgrad ....................................... True\n",
      "  global_batch_size ............................... 4\n",
      "  gradient_accumulation_fusion .................... True\n",
      "  group_query_attention ........................... False\n",
      "  head_lr_mult .................................... 1.0\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2560\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_h ........................................... 224\n",
      "  img_w ........................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  inference_batch_times_seqlen_threshold .......... 512\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  iter_per_epoch .................................. 1250\n",
      "  kv_channels ..................................... 80\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ None\n",
      "  local_rank ...................................... None\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 100\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_progress .................................... False\n",
      "  log_throughput .................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  log_world_size_to_tensorboard ................... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 5e-05\n",
      "  lr_decay_iters .................................. None\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. linear\n",
      "  lr_mult ......................................... 1.0\n",
      "  lr_warmup_fraction .............................. None\n",
      "  lr_warmup_init .................................. 0.0\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  manual_gc ....................................... False\n",
      "  manual_gc_eval .................................. True\n",
      "  manual_gc_interval .............................. 0\n",
      "  mask_factor ..................................... 1.0\n",
      "  mask_prob ....................................... 0.15\n",
      "  mask_type ....................................... random\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 2048\n",
      "  max_tokens_to_oom ............................... 12000\n",
      "  megatron_dataset_flag ........................... False\n",
      "  merge_file ...................................... /home/hice1/jho88/lab3B/src/merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 0.0\n",
      "  mock_data ....................................... False\n",
      "  model_return_dict ............................... True\n",
      "  model_type_name ................................. gpt\n",
      "  moe_aux_loss_coeff .............................. 0.0\n",
      "  moe_grouped_gemm ................................ False\n",
      "  moe_input_jitter_eps ............................ None\n",
      "  moe_router_load_balancing_type .................. aux_loss\n",
      "  moe_router_topk ................................. 2\n",
      "  moe_token_dropping .............................. False\n",
      "  moe_z_loss_coeff ................................ None\n",
      "  nccl_communicator_config_path ................... None\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_persist_layer_norm ........................... False\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  no_wd_decay_cond ................................ None\n",
      "  norm_epsilon .................................... 1e-05\n",
      "  normalization ................................... LayerNorm\n",
      "  num_attention_heads ............................. 32\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_experts ..................................... None\n",
      "  num_layers ...................................... 96\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_micro_batches ............................... 1\n",
      "  num_query_groups ................................ 1\n",
      "  num_workers ..................................... 2\n",
      "  one_logger_entity ............................... hwinf_dcm\n",
      "  one_logger_project .............................. e2e-tracking\n",
      "  one_logger_run_name ............................. None\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  orig_vocab_size ................................. 50257\n",
      "  output_bert_embeddings .......................... False\n",
      "  overlap_grad_reduce ............................. False\n",
      "  overlap_p2p_comm ................................ False\n",
      "  overlap_param_gather ............................ False\n",
      "  override_opt_param_scheduler .................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  perform_initialization .......................... True\n",
      "  pipeline_model_parallel_size .................... 2\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  position_embedding_type ......................... learned_absolute\n",
      "  pretraining_flag ................................ True\n",
      "  profile ......................................... False\n",
      "  profile_ranks ................................... [0]\n",
      "  profile_step_end ................................ 12\n",
      "  profile_step_start .............................. 10\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  recompute_granularity ........................... selective\n",
      "  recompute_method ................................ None\n",
      "  recompute_num_layers ............................ None\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  retro_add_retriever ............................. False\n",
      "  retro_attention_gate ............................ 1\n",
      "  retro_cyclic_train_iters ........................ None\n",
      "  retro_encoder_attention_dropout ................. 0.1\n",
      "  retro_encoder_hidden_dropout .................... 0.1\n",
      "  retro_encoder_layers ............................ 2\n",
      "  retro_num_neighbors ............................. 2\n",
      "  retro_num_retrieved_chunks ...................... 2\n",
      "  retro_return_doc_ids ............................ False\n",
      "  retro_verify_neighbor_count ..................... True\n",
      "  retro_workdir ................................... None\n",
      "  return_logits ................................... False\n",
      "  rotary_interleaved .............................. False\n",
      "  rotary_percent .................................. 1.0\n",
      "  rotary_seq_len_interpolation_factor ............. None\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ None\n",
      "  save_interval ................................... None\n",
      "  scale_lr_cond ................................... None\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sequence_parallel ............................... False\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  skip_train ...................................... False\n",
      "  spec ............................................ None\n",
      "  split ........................................... 969, 30, 1\n",
      "  squared_relu .................................... False\n",
      "  standalone_embedding_stage ...................... False\n",
      "  start_weight_decay .............................. 0.01\n",
      "  swiglu .......................................... False\n",
      "  swin_backbone_type .............................. tiny\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  test_data_path .................................. None\n",
      "  timing_log_level ................................ 0\n",
      "  timing_log_option ............................... minmax\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_model ................................. None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  tp_comm_bulk_dgrad .............................. True\n",
      "  tp_comm_bulk_wgrad .............................. True\n",
      "  tp_comm_overlap ................................. False\n",
      "  tp_comm_overlap_cfg ............................. None\n",
      "  tp_comm_split_ag ................................ True\n",
      "  tp_comm_split_rs ................................ True\n",
      "  train_data_path ................................. None\n",
      "  train_iters ..................................... 2\n",
      "  train_samples ................................... None\n",
      "  transformer_impl ................................ local\n",
      "  transformer_pipeline_model_parallel_size ........ 2\n",
      "  untie_embeddings_and_output_weights ............. False\n",
      "  use_checkpoint_args ............................. False\n",
      "  use_checkpoint_opt_param_scheduler .............. False\n",
      "  use_cpu_initialization .......................... None\n",
      "  use_distributed_optimizer ....................... False\n",
      "  use_flash_attn .................................. False\n",
      "  use_mcore_models ................................ False\n",
      "  use_one_sent_docs ............................... False\n",
      "  use_ring_exchange_p2p ........................... False\n",
      "  use_rotary_position_embeddings .................. False\n",
      "  valid_data_path ................................. None\n",
      "  variable_seq_lengths ............................ False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vision_backbone_type ............................ vit\n",
      "  vision_pretraining .............................. False\n",
      "  vision_pretraining_type ......................... classify\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/hice1/jho88/lab3B/src/vocab.json\n",
      "  vocab_size ...................................... None\n",
      "  wandb_exp_name .................................. \n",
      "  wandb_project ................................... \n",
      "  wandb_save_dir .................................. \n",
      "  weight_decay .................................... 0.01\n",
      "  weight_decay_incr_style ......................... constant\n",
      "  world_size ...................................... 2\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "torch distributed is already initialized, skipping initialization ...\n",
      "> initialized tensor model parallel with size 1\n",
      "> initialized pipeline model parallel with size 2\n",
      "> setting random seeds to 1234 ...\n",
      "> compiling dataset index builder ...\n",
      "make: Entering directory '/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/datasets'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/storage/ice-shared/ece8803hml/Megatron-LM/megatron/core/datasets'\n",
      ">>> done with dataset index builder. Compilation time: 0.047 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 0.882 seconds\n",
      "Building gpt model in the pre-training mode.\n",
      "The Megatron LM model weights are initialized at random in `accelerator.prepare`. Please use `accelerator.load_checkpoint` to load a pre-trained checkpoint matching the distributed setup.\n",
      "Preparing model optimizer scheduler\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 3910492160\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 3905254400\n",
      "Preparing dataloader\n",
      "> learning rate decay style: linear\n",
      "***** Running training *****\n",
      "Instantaneous batch size per device = 4\n",
      "Total train batch size = 4\n",
      "Total optimization steps = 2\n",
      "***** Running training *****\n",
      "Instantaneous batch size per device = 4\n",
      "Total train batch size = 4\n",
      "Total optimization steps = 2\n",
      "-------------------------------\n",
      "----- Training run failed -----\n",
      "-------------------------------\n",
      "Error: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 79.22 GiB of which 472.62 MiB is free. Including non-PyTorch memory, this process has 78.75 GiB memory in use. Of the allocated memory 72.62 GiB is allocated by PyTorch, and 1.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "## Try varying the number of layers and various parallelism strategies.\n",
    "!accelerate launch --main_process_port {find_freeport(25900)} --config_file \"megatron_configs/pipeline_parallel_w_activ_recomp.yaml\"  src/train.py --model_name \"model_configs/gpt3_27.json\" --total_batch_size \"4\" 2> /dev/null"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
