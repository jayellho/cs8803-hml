{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from system import *\n",
    "from analye_model import *\n",
    "from plot_rooflines import *\n",
    "from operators import SoftMax, layer_norm, GEMM, attention\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Layout \n",
    "Part 1 : Understanding various operators \n",
    "-  Writing number of operations for SoftMax, Batch Normalization, Q/K/V Multiplication, Attention - 3 point\n",
    "-  Writing data movement for SoftMax, Batch Normalization, Q/K/V Multiplication, Attention  - 1 point\n",
    "\n",
    "Part 2 : Runtime Computations - 1 points\n",
    "- Compute time \n",
    "- Memory time \n",
    "- Roofline time \n",
    "\n",
    "\n",
    "Part 3 : Building Neural Networks  - 1 point\n",
    "- Llama\n",
    "- gpt3\n",
    "\n",
    "\n",
    "Part 4 : Comparing the performance of NN on different HWs - 2 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.1 Various operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only run this once you have completed code in operators.py\n",
    "softmax1 = SoftMax([2, 256])\n",
    "softmax2 = SoftMax([254, 5])\n",
    "\n",
    "ln1 = layer_norm([2, 8, 512])\n",
    "ln2 = layer_norm([8, 128, 1024])\n",
    "\n",
    "gemm1 = GEMM([32, 16, 8, 32])\n",
    "gemm2 = GEMM([4, 128, 256, 128])\n",
    "\n",
    "attn1 = attention([3, 256, 96, 128])\n",
    "attn2 = attention([1, 256, 384, 12])\n",
    "\n",
    "with open('output_a1.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['softmax1', list(softmax1.get_tensors()), softmax1.get_num_ops()])\n",
    "    writer.writerow(['softmax2', list(softmax2.get_tensors()), softmax2.get_num_ops()])\n",
    "    writer.writerow(['ln1',  list(ln1.get_tensors()), ln1.get_num_ops()])\n",
    "    writer.writerow(['ln2',  list(ln2.get_tensors()), ln2.get_num_ops()])\n",
    "    writer.writerow(['gemm1', list(gemm1.get_tensors()), gemm1.get_num_ops()])\n",
    "    writer.writerow(['gemm2', list(gemm2.get_tensors()), gemm2.get_num_ops()])\n",
    "    writer.writerow(['attn1', list(attn1.get_tensors()), attn1.get_num_ops()])\n",
    "    writer.writerow(['attn2', list(attn2.get_tensors()), attn2.get_num_ops()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.2 Runtime Computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_network = [softmax1, softmax2, ln1, ln2, gemm1, gemm2, attn1, attn2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A100 https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf\n",
    "A100_GPU = System( offchip_mem_bw=1935,\n",
    "                   flops=312, frequency=1095 ,\n",
    "                   compute_efficiency=0.75, memory_efficiency=0.7)\n",
    "## https://developer.nvidia.com/embedded/jetson-modules\n",
    "jetson_nano = System( offchip_mem_bw=34, \n",
    "                 flops=20, frequency=625, \n",
    "                 compute_efficiency=0.85, memory_efficiency=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = analysis_model(example_network, A100_GPU)\n",
    "\n",
    "model_df.to_csv('output_a2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.3 Building Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO A.3.i : LLama 7B prefill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For reference only.\n",
    "batch_size = 2000\n",
    "example_network = [ layer_norm([batch_size, 5, 128]),\n",
    "            GEMM([batch_size, 8, 64, 512]),\n",
    "            GEMM([batch_size, 8, 16, 32]),\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_7B_prefill(batch_size):\n",
    "    ## Fill in the opertors of llama 7B-like prefill, please refer to the figure in pdf document.\n",
    "    ## Refer the example_network to follow the network declaration\n",
    "    model_arch = [\n",
    "                 ]\n",
    "    return model_arch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO A.3.ii : gpt3 175B decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt3_175B_decode(batch_size):\n",
    "    ## Fill in the opertors of gpt3 175B-like decode, please refer to the figure in pdf document.\n",
    "    ## Refer the example_network to follow the network declaration\n",
    "    model_arch = [\n",
    "    ]\n",
    "    return model_arch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llama_on_a100_df = analysis_model(llama_7B_prefill(64), A100_GPU)\n",
    "display(llama_on_a100_df)\n",
    "\n",
    "dot_roofline(llama_on_a100_df, A100_GPU)\n",
    "print(f'Total Cycles:{sum(llama_on_a100_df.loc[:, \"Cycles\"]):0.2f}, Total data (MB): {sum(llama_on_a100_df.loc[:, \"Total Data (MB)\"]):0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_on_a100_df = analysis_model(gpt3_175B_decode(64), A100_GPU)\n",
    "display(gpt3_on_a100_df)\n",
    "\n",
    "dot_roofline(gpt3_on_a100_df, A100_GPU)\n",
    "print(f'Total Cycles:{sum(gpt3_on_a100_df.loc[:, \"Cycles\"]):0.2f}, Total data (MB): {sum(gpt3_on_a100_df.loc[:, \"Total Data (MB)\"]):0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_on_a100_df.to_csv('output_a3i.csv', index=False)\n",
    "gpt3_on_a100_df.to_csv('output_a3ii.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.4 Compare on different Hardwares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO A.4.i\n",
    "Generate csv for llama and gpt3 on jetson nano system, with batch size 4. <br>\n",
    "Make sure to name the csv file 'output_a4i.csv' and 'output_a4ii.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO A.4.ii\n",
    "\n",
    "Comment on the change in operator behavior between systems? Do they change, if so why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO A.4.iii\n",
    "\n",
    "For running gpt3 175B decode, what changes would you suggest to on hardware specs that would help in optimizing the performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
